#!/usr/bin/env python3
"""
AI Model A/B Test Runner
AIãƒ¢ãƒ‡ãƒ«A/Bãƒ†ã‚¹ãƒˆå®Ÿè¡Œãƒ»ãƒ‡ãƒ¼ã‚¿åé›†ã‚·ã‚¹ãƒ†ãƒ 
"""

import asyncio
import json
import logging
import os
import sys
import uuid
from datetime import datetime, timedelta
from typing import Dict, List, Any


from core.ab_test_manager import (
    ab_test_manager, TestConfiguration, ModelType, TestStatus
)
from core.model_comparator import (
    model_comparator, ModelComparisonRequest
)
from core.database_adapter import db_adapter

# ãƒ­ã‚°è¨­å®š
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


class ABTestRunner:
    """A/Bãƒ†ã‚¹ãƒˆå®Ÿè¡Œã‚·ã‚¹ãƒ†ãƒ """
    
    def __init__(self):
        self.logger = logging.getLogger(__name__)
        self.test_data_samples = self._generate_test_samples()
    
    def _generate_test_samples(self) -> List[Dict[str, Any]]:
        """ãƒ†ã‚¹ãƒˆç”¨ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ"""
        return [
            # é‡‘åˆ©é–¢é€£ãƒ‹ãƒ¥ãƒ¼ã‚¹
            {
                'news': 'Federal Reserve raises interest rates by 0.25% to combat inflation',
                'pair': 'USD/JPY',
                'expected_direction': 'long',
                'category': 'monetary_policy'
            },
            {
                'news': 'Bank of Japan maintains ultra-loose monetary policy despite inflation',
                'pair': 'USD/JPY', 
                'expected_direction': 'long',
                'category': 'monetary_policy'
            },
            
            # é›‡ç”¨çµ±è¨ˆ
            {
                'news': 'US non-farm payrolls surge by 350,000, beating expectations',
                'pair': 'USD/JPY',
                'expected_direction': 'long',
                'category': 'employment'
            },
            {
                'news': 'Japan unemployment rate rises to 3.2%, higher than forecast',
                'pair': 'USD/JPY',
                'expected_direction': 'long', 
                'category': 'employment'
            },
            
            # ã‚¤ãƒ³ãƒ•ãƒ¬é–¢é€£
            {
                'news': 'US inflation accelerates to 4.2% year-on-year in latest reading',
                'pair': 'USD/JPY',
                'expected_direction': 'long',
                'category': 'inflation'
            },
            {
                'news': 'Japanese core CPI remains below 2% target for 12th month',
                'pair': 'USD/JPY',
                'expected_direction': 'long',
                'category': 'inflation'
            },
            
            # GDPãƒ»æˆé•·ç‡
            {
                'news': 'US GDP growth slows to 1.8% in Q3, below expectations',
                'pair': 'USD/JPY',
                'expected_direction': 'short',
                'category': 'growth'
            },
            {
                'news': 'Japan economy contracts 0.3% as consumer spending weakens',
                'pair': 'USD/JPY',
                'expected_direction': 'short',
                'category': 'growth'
            },
            
            # åœ°æ”¿å­¦ãƒªã‚¹ã‚¯
            {
                'news': 'Trade tensions escalate as new tariffs announced',
                'pair': 'USD/JPY',
                'expected_direction': 'short',
                'category': 'geopolitical'
            },
            {
                'news': 'Global supply chain disruptions continue to impact manufacturing',
                'pair': 'USD/JPY',
                'expected_direction': 'short',
                'category': 'geopolitical'
            },
            
            # ä¸­ç«‹çš„ãƒ‹ãƒ¥ãƒ¼ã‚¹
            {
                'news': 'Central bank officials express mixed views on future policy direction',
                'pair': 'USD/JPY',
                'expected_direction': 'hold',
                'category': 'neutral'
            },
            {
                'news': 'Economic indicators show mixed signals for future growth prospects',
                'pair': 'USD/JPY',
                'expected_direction': 'hold',
                'category': 'neutral'
            },
            
            # å¸‚å ´ãƒ‡ãƒ¼ã‚¿åˆ†æç”¨
            {
                'price_data': {
                    'open': 150.0,
                    'high': 151.5,
                    'low': 149.8,
                    'close': 151.2,
                    'volume': 100000
                },
                'pair': 'USD/JPY',
                'expected_direction': 'long',
                'category': 'technical'
            },
            {
                'price_data': {
                    'open': 150.0,
                    'high': 150.2,
                    'low': 148.5,
                    'close': 148.8,
                    'volume': 80000
                },
                'pair': 'USD/JPY',
                'expected_direction': 'short',
                'category': 'technical'
            },
            
            # EUR/USDç”¨ã‚µãƒ³ãƒ—ãƒ«
            {
                'news': 'European Central Bank hints at potential rate hikes next quarter',
                'pair': 'EUR/USD',
                'expected_direction': 'long',
                'category': 'monetary_policy'
            },
            {
                'news': 'Eurozone manufacturing PMI drops to 18-month low',
                'pair': 'EUR/USD',
                'expected_direction': 'short',
                'category': 'economic_data'
            }
        ]
    
    async def create_comprehensive_test(self) -> str:
        """åŒ…æ‹¬çš„ãªA/Bãƒ†ã‚¹ãƒˆã‚’ä½œæˆ"""
        try:
            test_id = f"comprehensive_test_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
            
            config = TestConfiguration(
                test_id=test_id,
                name="Comprehensive AI Model Performance Test",
                description="GPT-4, GPT-4-Turbo, O3-mini comparison for forex trading signals",
                models_to_test=[ModelType.GPT4, ModelType.GPT4_TURBO, ModelType.O3_MINI],
                traffic_split={
                    "gpt-4": 0.34,
                    "gpt-4-turbo": 0.33,
                    "o3-mini": 0.33
                },
                duration_days=7,
                min_samples=50,
                confidence_level=0.95,
                success_metrics=['accuracy', 'confidence', 'cost_efficiency', 'response_time']
            )
            
            success = await ab_test_manager.create_test(config)
            if success:
                self.logger.info(f"Created comprehensive test: {test_id}")
                return test_id
            else:
                self.logger.error("Failed to create test")
                return ""
                
        except Exception as e:
            self.logger.error(f"Failed to create comprehensive test: {e}")
            return ""
    
    async def run_model_comparison_batch(self, test_id: str, sample_count: int = 20) -> Dict[str, Any]:
        """ãƒ¢ãƒ‡ãƒ«æ¯”è¼ƒãƒãƒƒãƒå®Ÿè¡Œ"""
        try:
            # ãƒ†ã‚¹ãƒˆã‚µãƒ³ãƒ—ãƒ«ã‚’é¸æŠ
            selected_samples = self.test_data_samples[:sample_count]
            
            models_to_test = [ModelType.GPT4_TURBO, ModelType.O3_MINI]  # å®Ÿéš›ã«åˆ©ç”¨å¯èƒ½ãªãƒ¢ãƒ‡ãƒ«
            
            self.logger.info(f"Starting batch comparison with {len(selected_samples)} samples")
            
            # ãƒãƒƒãƒæ¯”è¼ƒå®Ÿè¡Œ
            results = await model_comparator.run_batch_comparison(
                test_cases=selected_samples,
                models=models_to_test,
                test_id=test_id
            )
            
            # çµæœåˆ†æ
            analysis = model_comparator.analyze_batch_results(results)
            
            self.logger.info(f"Batch comparison completed: {len(results)} comparisons")
            self.logger.info(f"Total cost: ${analysis.get('total_cost', 0):.4f}")
            
            return {
                'test_id': test_id,
                'results_count': len(results),
                'analysis': analysis,
                'individual_results': [r.to_dict() for r in results[:5]]  # æœ€åˆã®5ä»¶ã®ã¿
            }
            
        except Exception as e:
            self.logger.error(f"Batch comparison failed: {e}")
            return {'error': str(e)}
    
    async def simulate_actual_outcomes(self, test_id: str) -> int:
        """å®Ÿéš›ã®çµæœã‚’ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆï¼ˆæœ¬æ¥ã¯å®Ÿéš›ã®å¸‚å ´ãƒ‡ãƒ¼ã‚¿ï¼‰"""
        try:
            # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‹ã‚‰æœªå®Œäº†ã®çµæœã‚’å–å¾—
            query = """
            SELECT result_id, ai_response, request_data 
            FROM ab_test_results 
            WHERE test_id = ? AND actual_outcome IS NULL
            """
            
            results = await db_adapter.fetch_all_async(query, (test_id,))
            updated_count = 0
            
            for result in results:
                result_id = result[0]
                ai_response = json.loads(result[1]) if result[1] else {}
                request_data = json.loads(result[2]) if result[2] else {}
                
                # å®Ÿéš›ã®çµæœã‚’ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆ
                actual_outcome = self._simulate_market_outcome(ai_response, request_data)
                
                # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ›´æ–°
                success = await ab_test_manager.update_actual_outcome(result_id, actual_outcome)
                if success:
                    updated_count += 1
            
            self.logger.info(f"Updated {updated_count} results with simulated outcomes")
            return updated_count
            
        except Exception as e:
            self.logger.error(f"Failed to simulate outcomes: {e}")
            return 0
    
    def _simulate_market_outcome(self, ai_response: Dict[str, Any], request_data: Dict[str, Any]) -> Dict[str, Any]:
        """å¸‚å ´çµæœã®ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³"""
        # æœŸå¾…ã•ã‚Œã‚‹æ–¹å‘æ€§
        expected_direction = request_data.get('expected_direction', 'hold')
        
        # AIäºˆæ¸¬
        ai_direction = ai_response.get('direction', 'hold')
        ai_trade_warranted = ai_response.get('trade_warranted', False)
        
        # ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆã•ã‚ŒãŸå¸‚å ´ã®å‹•ã
        if expected_direction == 'long':
            simulated_return = 0.015  # 1.5%ã®ä¸Šæ˜‡
        elif expected_direction == 'short':
            simulated_return = -0.012  # 1.2%ã®ä¸‹è½
        else:  # hold
            simulated_return = 0.003  # 0.3%ã®å°ã•ãªå¤‰å‹•
        
        # P&Lè¨ˆç®—
        if ai_trade_warranted:
            if ai_direction == 'long':
                profit_loss = simulated_return
            elif ai_direction == 'short':
                profit_loss = -simulated_return
            else:
                profit_loss = 0.0
        else:
            profit_loss = 0.0  # ãƒˆãƒ¬ãƒ¼ãƒ‰ã—ãªã„å ´åˆ
        
        # æ­£ç¢ºæ€§åˆ¤å®š
        direction_correct = (
            (expected_direction == 'long' and ai_direction == 'long') or
            (expected_direction == 'short' and ai_direction == 'short') or
            (expected_direction == 'hold' and not ai_trade_warranted)
        )
        
        return {
            'simulated_return': simulated_return,
            'profit_loss': profit_loss,
            'direction_correct': direction_correct,
            'expected_direction': expected_direction,
            'ai_direction': ai_direction,
            'simulation_timestamp': datetime.now().isoformat()
        }
    
    async def generate_test_report(self, test_id: str) -> Dict[str, Any]:
        """ãƒ†ã‚¹ãƒˆçµæœãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ"""
        try:
            # ãƒ†ã‚¹ãƒˆã‚µãƒãƒªãƒ¼ç”Ÿæˆ
            summary = await ab_test_manager.generate_test_summary(test_id)
            
            if not summary:
                return {'error': 'Failed to generate summary'}
            
            # è¿½åŠ åˆ†æ
            query = """
            SELECT model_used, 
                   COUNT(*) as total_requests,
                   AVG(confidence_score) as avg_confidence,
                   AVG(cost_usd) as avg_cost,
                   AVG(processing_time_ms) as avg_time,
                   COUNT(CASE WHEN success = 1 THEN 1 END) as success_count
            FROM ab_test_results 
            WHERE test_id = ?
            GROUP BY model_used
            """
            
            model_stats = await db_adapter.fetch_all_async(query, (test_id,))
            
            # ã‚«ãƒ†ã‚´ãƒªåˆ¥åˆ†æ
            category_query = """
            SELECT json_extract(request_data, '$.category') as category,
                   model_used,
                   AVG(confidence_score) as avg_confidence,
                   COUNT(CASE WHEN success = 1 THEN 1 END) as success_count,
                   COUNT(*) as total_count
            FROM ab_test_results 
            WHERE test_id = ? AND json_extract(request_data, '$.category') IS NOT NULL
            GROUP BY category, model_used
            """
            
            category_stats = await db_adapter.fetch_all_async(category_query, (test_id,))
            
            report = {
                'test_id': test_id,
                'summary': summary.to_dict(),
                'model_statistics': [
                    {
                        'model': stat[0],
                        'total_requests': stat[1],
                        'avg_confidence': stat[2],
                        'avg_cost': stat[3],
                        'avg_time_ms': stat[4],
                        'success_count': stat[5],
                        'success_rate': stat[5] / stat[1] if stat[1] > 0 else 0
                    }
                    for stat in model_stats
                ],
                'category_analysis': [
                    {
                        'category': stat[0],
                        'model': stat[1],
                        'avg_confidence': stat[2],
                        'success_count': stat[3],
                        'total_count': stat[4],
                        'success_rate': stat[3] / stat[4] if stat[4] > 0 else 0
                    }
                    for stat in category_stats
                ],
                'generated_at': datetime.now().isoformat()
            }
            
            return report
            
        except Exception as e:
            self.logger.error(f"Failed to generate test report: {e}")
            return {'error': str(e)}
    
    async def run_complete_test_cycle(self, sample_count: int = 15) -> Dict[str, Any]:
        """å®Œå…¨ãªãƒ†ã‚¹ãƒˆã‚µã‚¤ã‚¯ãƒ«ã‚’å®Ÿè¡Œ"""
        try:
            self.logger.info("=== Starting Complete A/B Test Cycle ===")
            
            # 1. ãƒ†ã‚¹ãƒˆä½œæˆ
            test_id = await self.create_comprehensive_test()
            if not test_id:
                return {'error': 'Failed to create test'}
            
            # 2. ãƒ¢ãƒ‡ãƒ«æ¯”è¼ƒå®Ÿè¡Œ
            comparison_results = await self.run_model_comparison_batch(test_id, sample_count)
            
            # 3. å®Ÿéš›ã®çµæœã‚’ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆ
            await asyncio.sleep(2)  # å°‘ã—å¾…æ©Ÿ
            updated_outcomes = await self.simulate_actual_outcomes(test_id)
            
            # 4. ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
            report = await self.generate_test_report(test_id)
            
            self.logger.info("=== A/B Test Cycle Complete ===")
            
            return {
                'test_id': test_id,
                'comparison_results': comparison_results,
                'updated_outcomes': updated_outcomes,
                'final_report': report,
                'success': True
            }
            
        except Exception as e:
            self.logger.error(f"Complete test cycle failed: {e}")
            return {'error': str(e), 'success': False}


async def initialize_database():
    """ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹åˆæœŸåŒ–"""
    try:
        # ã‚¹ã‚­ãƒ¼ãƒä½œæˆ
        with open('sql/ab_test_schema.sql', 'r') as f:
            schema_sql = f.read()
        
        # ã‚»ãƒŸã‚³ãƒ­ãƒ³ã§åˆ†å‰²ã—ã¦å®Ÿè¡Œ
        statements = [stmt.strip() for stmt in schema_sql.split(';') if stmt.strip()]
        
        for statement in statements:
            if statement and not statement.startswith('--'):
                try:
                    await db_adapter.execute_async(statement)
                except Exception as e:
                    if "already exists" not in str(e):
                        logger.warning(f"Failed to execute SQL statement: {e}")
        
        logger.info("Database initialized successfully")
        return True
        
    except Exception as e:
        logger.error(f"Database initialization failed: {e}")
        return False


async def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°"""
    logger.info("AI Model A/B Test Runner Starting...")
    
    try:
        # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹åˆæœŸåŒ–
        db_success = await initialize_database()
        if not db_success:
            logger.error("Database initialization failed")
            return False
        
        # ãƒ†ã‚¹ãƒˆãƒ©ãƒ³ãƒŠãƒ¼ä½œæˆ
        test_runner = ABTestRunner()
        
        # å®Œå…¨ãªãƒ†ã‚¹ãƒˆã‚µã‚¤ã‚¯ãƒ«å®Ÿè¡Œ
        results = await test_runner.run_complete_test_cycle(sample_count=12)
        
        if results.get('success', False):
            logger.info("ğŸ‰ A/B Test completed successfully!")
            
            # çµæœä¿å­˜
            output_file = f"ab_test_results_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
            with open(output_file, 'w') as f:
                json.dump(results, f, indent=2, default=str)
            
            logger.info(f"Results saved to {output_file}")
            
            # ã‚µãƒãƒªãƒ¼è¡¨ç¤º
            if 'final_report' in results and 'model_statistics' in results['final_report']:
                logger.info("\n=== MODEL PERFORMANCE SUMMARY ===")
                for stat in results['final_report']['model_statistics']:
                    logger.info(
                        f"{stat['model']}: "
                        f"Success Rate: {stat['success_rate']:.1%}, "
                        f"Avg Cost: ${stat['avg_cost']:.4f}, "
                        f"Avg Confidence: {stat['avg_confidence']:.3f}"
                    )
            
            return True
        else:
            logger.error("âŒ A/B Test failed")
            logger.error(f"Error: {results.get('error', 'Unknown error')}")
            return False
            
    except Exception as e:
        logger.error(f"Main execution failed: {e}")
        return False


if __name__ == "__main__":
    success = asyncio.run(main())
    sys.exit(0 if success else 1)