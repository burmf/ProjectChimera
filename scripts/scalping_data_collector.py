#!/usr/bin/env python3
"""
Bitget ã‚¹ã‚­ãƒ£ãƒ«ãƒ”ãƒ³ã‚°ãƒ‡ãƒ¼ã‚¿åé›†å®Ÿè¡Œã‚¹ã‚¯ãƒªãƒ—ãƒˆ
1ãƒ¶æœˆåˆ†ã®é«˜é »åº¦ãƒ‡ãƒ¼ã‚¿(1åˆ†ã€5åˆ†ã€15åˆ†è¶³)ã‚’åé›†ã—ã€
ãƒ†ã‚¯ãƒ‹ã‚«ãƒ«åˆ†æã¨ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°ã‚’å®Ÿè¡Œ
"""

import sys
import os
import json
import pickle
import logging
from datetime import datetime, timedelta
import pandas as pd
import numpy as np

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ‘ã‚¹ã‚’è¿½åŠ 
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from core.bitget_rest_client import BitgetRestClient, ScalpingDataCollector
from core.database_adapter import db_adapter
from core.logging_config import setup_logging

# ãƒ­ã‚°è¨­å®š
setup_logging()
logger = logging.getLogger(__name__)

def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°"""
    logger.info("ğŸš€ Bitget ã‚¹ã‚­ãƒ£ãƒ«ãƒ”ãƒ³ã‚°ãƒ‡ãƒ¼ã‚¿åé›†é–‹å§‹")
    
    # ãƒ‡ãƒ¼ã‚¿åé›†å™¨ã‚’åˆæœŸåŒ–
    collector = ScalpingDataCollector(database_adapter=db_adapter)
    
    try:
        # 1ãƒ¶æœˆåˆ†ã®ãƒ‡ãƒ¼ã‚¿ã‚’åé›†
        logger.info("ğŸ“Š 1ãƒ¶æœˆåˆ†ã®é«˜é »åº¦ãƒ‡ãƒ¼ã‚¿åé›†ä¸­...")
        dataset = collector.collect_scalping_dataset(days_back=30)
        
        # ãƒ‡ãƒ¼ã‚¿å“è³ªãƒã‚§ãƒƒã‚¯
        total_records = 0
        data_summary = {}
        
        for pair, timeframes in dataset.items():
            pair_records = 0
            pair_summary = {}
            
            for timeframe, df in timeframes.items():
                records = len(df) if not df.empty else 0
                pair_records += records
                pair_summary[timeframe] = {
                    'records': records,
                    'date_range': {
                        'start': df['timestamp'].min().isoformat() if not df.empty else None,
                        'end': df['timestamp'].max().isoformat() if not df.empty else None
                    },
                    'avg_volume': float(df['volume'].mean()) if not df.empty else 0,
                    'price_range': {
                        'min': float(df['low'].min()) if not df.empty else 0,
                        'max': float(df['high'].max()) if not df.empty else 0
                    }
                }
            
            total_records += pair_records
            data_summary[pair] = pair_summary
            logger.info(f"âœ… {pair}: {pair_records:,} ãƒ¬ã‚³ãƒ¼ãƒ‰åé›†å®Œäº†")
        
        logger.info(f"ğŸ¯ ç·åé›†ãƒ¬ã‚³ãƒ¼ãƒ‰æ•°: {total_records:,}")
        
        # ãƒ‡ãƒ¼ã‚¿ã‚’ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜
        output_dir = "/home/ec2-user/ProjectChimera/data/scalping"
        os.makedirs(output_dir, exist_ok=True)
        
        # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆä¿å­˜
        dataset_file = os.path.join(output_dir, f"scalping_dataset_{datetime.now().strftime('%Y%m%d_%H%M%S')}.pkl")
        with open(dataset_file, 'wb') as f:
            pickle.dump(dataset, f)
        logger.info(f"ğŸ’¾ ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆä¿å­˜: {dataset_file}")
        
        # ã‚µãƒãƒªãƒ¼ä¿å­˜
        summary_file = os.path.join(output_dir, f"data_summary_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json")
        with open(summary_file, 'w') as f:
            json.dump(data_summary, f, indent=2, default=str)
        logger.info(f"ğŸ“‹ ã‚µãƒãƒªãƒ¼ä¿å­˜: {summary_file}")
        
        # ã‚¹ã‚­ãƒ£ãƒ«ãƒ”ãƒ³ã‚°ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ†æ
        logger.info("ğŸ” ã‚¹ã‚­ãƒ£ãƒ«ãƒ”ãƒ³ã‚°ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ†æä¸­...")
        pattern_analysis = collector.analyze_scalping_patterns(dataset)
        
        # åˆ†æçµæœä¿å­˜
        analysis_file = os.path.join(output_dir, f"pattern_analysis_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json")
        with open(analysis_file, 'w') as f:
            json.dump(pattern_analysis, f, indent=2, default=str)
        logger.info(f"ğŸ“ˆ ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ†æä¿å­˜: {analysis_file}")
        
        # çµæœã‚µãƒãƒªãƒ¼è¡¨ç¤º
        print_collection_summary(data_summary, pattern_analysis)
        
        logger.info("ğŸ‰ ã‚¹ã‚­ãƒ£ãƒ«ãƒ”ãƒ³ã‚°ãƒ‡ãƒ¼ã‚¿åé›†å®Œäº†!")
        
    except Exception as e:
        logger.error(f"âŒ ãƒ‡ãƒ¼ã‚¿åé›†ã‚¨ãƒ©ãƒ¼: {e}")
        raise

def print_collection_summary(data_summary: dict, pattern_analysis: dict):
    """åé›†çµæœã‚µãƒãƒªãƒ¼ã‚’è¡¨ç¤º"""
    print("\n" + "="*80)
    print("ğŸ“Š BITGET ã‚¹ã‚­ãƒ£ãƒ«ãƒ”ãƒ³ã‚°ãƒ‡ãƒ¼ã‚¿åé›†çµæœ")
    print("="*80)
    
    for pair, timeframes in data_summary.items():
        print(f"\nğŸ”¸ {pair}")
        total_pair_records = sum(tf['records'] for tf in timeframes.values())
        print(f"   ç·ãƒ¬ã‚³ãƒ¼ãƒ‰æ•°: {total_pair_records:,}")
        
        for timeframe, info in timeframes.items():
            print(f"   {timeframe:>3}: {info['records']:>6,} ãƒ¬ã‚³ãƒ¼ãƒ‰")
            if info['date_range']['start']:
                start_date = datetime.fromisoformat(info['date_range']['start']).strftime('%m/%d %H:%M')
                end_date = datetime.fromisoformat(info['date_range']['end']).strftime('%m/%d %H:%M')
                print(f"        æœŸé–“: {start_date} - {end_date}")
                print(f"        ä¾¡æ ¼: ${info['price_range']['min']:.4f} - ${info['price_range']['max']:.4f}")
    
    print(f"\nğŸ“ˆ ã‚¹ã‚­ãƒ£ãƒ«ãƒ”ãƒ³ã‚°åˆ†æçµæœ:")
    
    for pair, timeframes in pattern_analysis.items():
        print(f"\nğŸ¯ {pair} æœ€é©åŒ–æƒ…å ±:")
        
        for timeframe, analysis in timeframes.items():
            if not analysis:
                continue
                
            print(f"   {timeframe}:")
            print(f"     å¹³å‡ãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£: {analysis.get('avg_volatility', 0):.4f}")
            print(f"     é«˜å‡ºæ¥é«˜é–¾å€¤: {analysis.get('high_volume_threshold', 0):.2f}")
            print(f"     å¤§ããªå€¤å‹•ã: {analysis.get('significant_moves_count', 0):,} å›")
            
            best_hours = analysis.get('best_scalping_hours', [])
            if best_hours:
                print(f"     æœ€é©æ™‚é–“å¸¯: {', '.join(f'{h:02d}:00' for h in best_hours)}")
    
    print("\n" + "="*80)

if __name__ == "__main__":
    main()